# Macroeconomic Event Fetching, Validation, and Sentiment Analysis

This directory contains Python scripts and associated data for fetching macroeconomic events, validating their authenticity, performing sentiment analysis using various models (including OpenAI's API), and analyzing their potential market impact.

## Overview

The primary workflow involves:
1.  Fetching historical events for specific dates using an OpenAI model (`events.py`).
2.  Performing sentiment analysis on the event descriptions using multiple NLP models (`sentiment.py`).
3.  Merging sentiment scores with market return data for correlation and impact analysis (`sentiment.py`).
4.  Optionally validating fetched events using another OpenAI model (`validator.py`).
5.  Optionally correcting specific event entries if reprocessing was needed (`postprocess.py`).

## Key Scripts

- **`events.py`**:
    - **Objective**: Fetches 18 significant historical events (International, National for HK/SK/US/JP, Social/Tech) per specified date using OpenAI's API.
    - **Model**: Configured to use an OpenAI model (e.g., `gpt-4o-mini` as per `CFG.MODEL_NAME`).
    - **Prompt**: Includes detailed instructions for the model, specifying categories, fallback logic (checking previous days), JSON output schema, and handling of weekend/holiday events (see `SYSTEM_MESSAGE`).
    - **Parameters**: Uses `max_tokens` (e.g., 4096), enforces JSON output (`response_format`), and sets a low `temperature` (e.g., 0.2) for factual recall.
    - **Functionality**: Supports processing date ranges via batch API (`process_batch`) or specific date lists via direct API calls (`process_direct_dates`) or batch API calls (`process_batch_dates`). Monitors batch job progress.
    - **Output**: Saves generated tasks to `Saves/` and processed outputs to `Processed_files/` in `.jsonl` format.

- **`sentiment.py`**: 
    - **Objective**: Performs comprehensive sentiment analysis on parsed event descriptions and correlates results with stock market returns.
    - **Sentiment Models**: Primarily uses Transformer-based models:
        - `ProsusAI/finbert` (Financial Domain)
        - `cardiffnlp/twitter-roberta-base-sentiment` (General/Social)
    - **Functionality**:
        - Initializes and runs transformer models on available GPU (CUDA) or CPU.
        - Processes transformer models in batches for efficiency.
        - Cleans event descriptions (removes date prefixes).
        - Downloads historical stock data (`yfinance`) for specified tickers (`^HSI`, `^KS11`, `^GSPC`, `^N225`) and calculates daily & future returns (1d, 5d, 20d).
        - Merges sentiment scores with market data.
        - Performs keyword analysis (noun extraction via `nltk`, TF-IDF, sentiment aggregation per keyword).
        - Conducts market analysis: Correlation (Pearson), Cross-Correlation (CCF), Granger Causality between sentiment and returns.
        - Calculates optimal weights for sentiment tools (FinBERT, RoBERTa) to predict future returns using Linear Regression.
    - **Caching**: Uses `.pkl` files in `Saves/` to cache intermediate results (parsed events, sentiment scores, stock data, keyword analysis) to speed up re-runs (`force_update=True` in `CFG` class overrides caching).
    - **Visualization**: Generates plots (saved to `Visualizations/`) for sentiment distributions, correlations, sentiment over time, keyword analysis, market impact, and optimal weights.

- **`postprocess.py`**: 
    - **Objective**: Updates the main event aggregation file (`BatchAggreg.jsonl`) with corrected data from a re-processing batch (`BatchRecollect_tasks_output.jsonl`).
    - **Use Case**: Used specifically during the project to fix entries for dates that were processed incorrectly in the initial run. This is an optional step for general use.

- **`validator.py`**:
    - **Objective**: Validates the historical authenticity of events fetched by `events.py` and attempts to find reliable reference links using OpenAI.
    - **Use Case**: Used during the project for data quality assurance. This is an optional step if only event fetching and sentiment analysis are required.
    - **Model**: Configured to use an OpenAI model (e.g., `o3-mini-2025-01-31`).
    - **Prompt**: Instructs the model to verify events against historical records and provide reference links, returning null if verification fails (see `SYSTEM_MESSAGE_VALIDATION`).
    - **Input**: Reads `Batch*_output.jsonl` files (output from `events.py`).
    - **Output**: Saves validation results (original event + reference link) to `*_validation_output.jsonl` files.

## Data Files

- **`.jsonl` files**:
    - `*tasks.jsonl` (in `Saves/`): Input files prepared for OpenAI batch API requests (generated by `events.py`).
    - `*_output.jsonl` (in `Processed_files/`): Output files received from the OpenAI API containing the generated event data (output of `events.py`).
    - `BatchAggreg.jsonl`: The primary aggregation of event data outputs, potentially updated by `postprocess.py`.
    - `BatchRecollect*.jsonl`: Files related to the reprocessing/recollection workflow managed by `postprocess.py`.
    - `*_validation_output.jsonl`: Output from `validator.py` containing original events and validation reference links.
    - **Disclaimer:** These files contain the specific prompts sent to the OpenAI models and the corresponding responses received, potentially revealing details about the analysis methodology and data used in the original capstone project.
- **`.pkl` files** (in `Saves/`):
    - `parsed_events.pkl`: Cached DataFrame containing structured event data parsed from the primary `.jsonl` output (e.g., `BatchAggreg.jsonl`).
    - `sentiments.pkl`: Cached DataFrame containing sentiment scores from various tools (now primarily FinBERT, RoBERTa).
    - `stock_data.pkl`: Cached dictionary of DataFrames containing downloaded stock market data.
    - `keyword_sentiments.pkl`: Cached DataFrame containing aggregated sentiment scores per extracted keyword.
    - Other `.pkl` files might be generated for specific keyword analyses per tool/country.
- **`Saves/`**: Directory used for saving intermediate data, task files, and cached `.pkl` files.
- **`Processed_files/`**: Directory containing the final processed output `.jsonl` files from OpenAI batch jobs (`events.py`).
- **`Visualizations/`**: Directory containing all plots generated by `sentiment.py`.

## Configuration

- **`api_key.txt`**: **Crucial.** This file must contain your OpenAI API key on a single line. It is read by `events.py` and `validator.py`. **Do not commit this file with your actual key to version control.**
- **`CFG` Class (within scripts)**: Internal classes (e.g., in `events.py`, `sentiment.py`) define parameters like `MODEL_NAME`, `MAX_COMPLETION_TOKENS`, `force_update`, file paths etc.

## Code Disclaimer

The Python scripts (`events.py`, `sentiment.py`, `validator.py`) contain the specific prompts and logic used to interact with the OpenAI API for the capstone project. Reviewing the code will provide insight into the data processing, validation, sentiment analysis, and market correlation techniques employed.

## Usage Workflow

1.  **Setup**: Place your OpenAI API key in `api_key.txt`.
2.  **Fetch Events**: Run `python Macro/Events/events.py`. This script uses the OpenAI API (check `CFG` class for model/parameters) to generate historical events and saves the output `.jsonl` file(s) in `Processed_files/`.
3.  **Analyze Sentiment & Market Impact**: Run `python Macro/Events/sentiment.py`. This script:
    - Parses the primary event output file (defined in `CFG.input_jsonl`, likely `BatchAggreg.jsonl`).
    - Calculates sentiment scores using FinBERT and RoBERTa (caches results in `Saves/`).
    - Downloads stock data (caches results in `Saves/`).
    - Performs keyword analysis, market correlation, Granger causality, etc.
    - Generates numerous plots saved in `Visualizations/`.
4.  **(Optional - Validation)**: Run `python Macro/Events/validator.py` *after* step 2 if you need to validate the fetched events. It processes the output(s) from `events.py` using the OpenAI API for validation.
5.  **(Optional - Correction)**: If specific dates needed reprocessing (using `recollect.jsonl` and `events.py`'s RECOLLECT_MODE), run `python Macro/Events/postprocess.py` *after* the recollection batch job completes and *before* running `sentiment.py` (step 3) to update `BatchAggreg.jsonl`. 